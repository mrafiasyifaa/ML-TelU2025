{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51c3ac50",
   "metadata": {},
   "source": [
    "# ðŸ“ **Chapter 4 - Training Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9706a56",
   "metadata": {},
   "source": [
    "# Excercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf87e899",
   "metadata": {},
   "source": [
    "## 1. Which Linear Regression training algorithm can you use if you have a training set with millions of features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeeae2e",
   "metadata": {},
   "source": [
    "> Jutaan fitur dapat ditangani menggunakan ***Gradient Descent*** atau variannya (*Stochaastic* atau *Mini-batch* GD). <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c684954",
   "metadata": {},
   "source": [
    "> Apabila menggunakan selain *Gradient Descent*, seperti *Normal Equation* atau SVD, model dapat memiliki kompleksitas yang meningkat secara drastis seiring bertambahnya gitur dan membuat algoritma menjadi lambat. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3dd57d",
   "metadata": {},
   "source": [
    "## 2. Suppose the features in your training set have very different scales. Which algorithms might suffer from this, and how? What can you do about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e497f3e",
   "metadata": {},
   "source": [
    "> - Algoritma *Gradient Descent* beserta semua variannya akan menderita karena mereka sensitif terhadap skala fitur\n",
    "> - Solusi untuk *Gradient Descent* dapat melakukan *feature scaling* menggunakan *StandardScaler* dari *Scikit-Learn*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217bf964",
   "metadata": {},
   "source": [
    "> - *Normal Equation* atau SVD akan memberi hasil yang sub-optimal tanpa *feature scaling*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911dc58b",
   "metadata": {},
   "source": [
    "## 3. Can Gradient Descent get stuck in a local minimum when training a Logistic Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fb413d",
   "metadata": {},
   "source": [
    "> *Gradient Descent* memiliki fungsi *cost* yang cembung. Fungsi yang cembung berciri memiliki global minimum dan tidak memiliki lokal minimum sehingga **mustahil** Algoritma GD mengalami *stuck*. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e4c1e",
   "metadata": {},
   "source": [
    "## 4. Do all Gradient Descent algorithms lead to the same model, provided you let them run long enough?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f891e1b",
   "metadata": {},
   "source": [
    "> Semua algoritma GD akan menghasilkan model yang sedikit berbeda : <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b5319f",
   "metadata": {},
   "source": [
    "> - *Batch Gradient Descent* akan konvergen ke solusi optimal. <br>\n",
    "> - *Stochastic GD* dan *Mini-batch* jika menggunakan *learning rate* dengan nilai yang tetap, mereka akan menghasilkan model yang tidak konvergen, tetapi memantul di sekitar minimum. Jika *Stochastic* dan *Mini-batch* menggunakan nilai *learning rate* yang kecil secara bertahap, penerapan ini dapat menghasilkan model yang menetap di global minimum. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ba0629",
   "metadata": {},
   "source": [
    "## 5. Suppose you use Batch Gradient Descent and you plot the validation error at every epoch. If you notice that the validation error consistently goes up, what is likely going on? How can you fix this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7720decf",
   "metadata": {},
   "source": [
    "> - Model kemungkinan memiliki *learning rate* terlalu tinggi sehingga menghasilkan fungsi yang tidak konvergen. <br>\n",
    "> - Kemungkinan kedua adalah model mengalami *overfitting* pada data training. Model dapat menggunakan *early stopping* ketika error pada *validation* mulai mengalami kenaikan dari *epoch* sebelumnya. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fcfc21",
   "metadata": {},
   "source": [
    "## 6. Is it a good idea to stop Mini-batch Gradient Descent immediately when the validation error goes up?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa932a48",
   "metadata": {},
   "source": [
    "> Karena sifatnya yang *stochastic* (acak), menghentikan pelatihan model adalah **ide yang buruk**. Sangat wajar jika pelatihan yang sifatnya acak akan memiliki error yang tidak mulus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df8f3a7",
   "metadata": {},
   "source": [
    "## 7. Which Gradient Descent algorithm (among those we discussed) will reach the vicinity of the optimal solution the fastest? Which will actually converge? How can you make the others converge as well?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954a7e83",
   "metadata": {},
   "source": [
    "> - *Stochastic Gradient Descent* akan mencapai daerah optimal global terlebih dahulu akibat algoritmanya yang memproses satu *instance* per epoch model mengambil langkah training lebih frekuens. <br>\n",
    "> - *Mini-batch Gradient* juga bisa dengan cepat mencapai daerah global optimal jika *batch*nya kecil. <br>\n",
    "> - Untuk *Stochastic* dan *Mini-batch* dapat konvergen jika melakukan penerapan *learning rate* yang dikurangi secara bertahap. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d702c65",
   "metadata": {},
   "source": [
    "## 8. Suppose you are using Polynomial Regression. You plot the learning curves and you notice that there is a large gap between the training error and the validation error. What is happening? What are three ways to solve this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9a1dc7",
   "metadata": {},
   "source": [
    "> Model mengalami *overfitting*. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e66502",
   "metadata": {},
   "source": [
    "> - Dapat menambah lebih banyak data training. <br>\n",
    "> - Menerapkan regularisasi pada model. <br>\n",
    "> - Mengurangi kompleksitas model. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb3988e",
   "metadata": {},
   "source": [
    "## 9. Suppose you are using Ridge Regression and you notice that the training error and the validation error are almost equal and fairly high. Would you say that the model suffers from high bias or high variance? Should you increase the regularization hyperparameter Î± or reduce it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca86e218",
   "metadata": {},
   "source": [
    "> Model mengalami *high bias* (*underfitting*). <br>\n",
    "> Ini diakibatkan oleh nilai Î± yang tinggi. Memaksa model menjadi lebih sederhana dan meningkatkan *bias*. <br>\n",
    "> Model perlu dibuat fit dengan mengurangi nilai Î±. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576274fe",
   "metadata": {},
   "source": [
    "## 10. Why would you want to use:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d899bb",
   "metadata": {},
   "source": [
    "### a. Ridge Regression instead of plain Linear Regression (i.e., without any regularization)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcf5b91",
   "metadata": {},
   "source": [
    "> Selalu utamakan *Ridge Regression* daripada *Linear Regression*. <br>\n",
    "> Regulariasi (seperti menggunakan *Ridge*) lebih baik digunakan walau sedikit taripada tidak ada regularisasi sama sekali. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78c9758",
   "metadata": {},
   "source": [
    "### b. Lasso instead of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d82afa7",
   "metadata": {},
   "source": [
    "> *Lasso* mengurangi bobot dari fitur yang dianggap tidak penting menjadi nol. <br\n",
    "> Gunakan *Lasso* ketika Anda curiga bahwa hanya terdapat sedikit fitur yang penting untuk model. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a3e4cc",
   "metadata": {},
   "source": [
    "### c. Elastic Net instead of Lasso?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e99b1ca",
   "metadata": {},
   "source": [
    "> *Elastic Net* digunakan untuk mencapai kestabilan ketika fitur-fitur berjumlah banyak dan saling berkorelasi kuat. <br>\n",
    "> Jika jumlah *instance* training lebih kecil dari jumlah fitur, *Lasso* akan tidak stabil, sedangkan *Elastic Net* stabil. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14600461",
   "metadata": {},
   "source": [
    "## 11. Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime. Should you implement two Logistic Regression classifiers or one Softmax Regression classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66730db7",
   "metadata": {},
   "source": [
    "> - *Softmac Regression* digunakan untuk kelas yang saling eksklusif. Model hanya bisa memilih satu kelas dari banyak label. <br>\n",
    "> - ***Logistic Regression* cocok untuk permasalahan ini** karena masing masing klasifikasi dapat dibagi menjadi dua tugas *binary*. Digit biner pertama untuk *outdoor/indoor* dan digit kedua untuk *daytime/nighttime*. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121f3bae",
   "metadata": {},
   "source": [
    "## 12. Implement Batch Gradient Descent with early stopping for Softmax Regression (without using Scikit-Learn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dc0b411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memulai training...\n",
      "Epoch: 0, Validation Loss: 4.6320\n",
      "Epoch: 500, Validation Loss: 0.5574\n",
      "Epoch: 1000, Validation Loss: 0.4403\n",
      "Epoch: 1500, Validation Loss: 0.3687\n",
      "Epoch: 2000, Validation Loss: 0.3193\n",
      "Epoch: 2500, Validation Loss: 0.2831\n",
      "Epoch: 3000, Validation Loss: 0.2556\n",
      "Epoch: 3500, Validation Loss: 0.2339\n",
      "Epoch: 4000, Validation Loss: 0.2165\n",
      "Epoch: 4500, Validation Loss: 0.2021\n",
      "Epoch: 5000, Validation Loss: 0.1901\n",
      "\n",
      "Training selesai.\n",
      "Loss validasi terbaik: 0.1901\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"]\n",
    "y = iris[\"target\"]\n",
    "\n",
    "X_with_bias = np.c_[np.ones([len(X), 1]), X]\n",
    "\n",
    "def to_one_hot(y):\n",
    "    n_classes = y.max() + 1\n",
    "    m = len(y)\n",
    "    Y_one_hot = np.zeros((m, n_classes))\n",
    "    Y_one_hot[np.arange(m), y] = 1\n",
    "    return Y_one_hot\n",
    "\n",
    "Y_one_hot = to_one_hot(y)\n",
    "\n",
    "test_ratio = 0.33\n",
    "validation_size = int(len(X_with_bias) * test_ratio)\n",
    "train_size = len(X_with_bias) - validation_size\n",
    "\n",
    "np.random.seed(42)\n",
    "shuffled_indices = np.random.permutation(len(X_with_bias))\n",
    "\n",
    "X_train = X_with_bias[shuffled_indices[:train_size]]\n",
    "Y_train = Y_one_hot[shuffled_indices[:train_size]]\n",
    "X_val = X_with_bias[shuffled_indices[train_size:]]\n",
    "Y_val = Y_one_hot[shuffled_indices[train_size:]]\n",
    "\n",
    "def softmax(logits):\n",
    "    exps = np.exp(logits)\n",
    "    exp_sums = np.sum(exps, axis=1, keepdims=True)\n",
    "    return exps / exp_sums\n",
    "\n",
    "def cross_entropy_loss(Y_proba, Y_target):\n",
    "    m = len(Y_target)\n",
    "    epsilon = 1e-7\n",
    "    log_likelihood = -np.log(Y_proba[range(m), Y_target.argmax(axis=1)] + epsilon)\n",
    "    return np.sum(log_likelihood) / m\n",
    "\n",
    "n_inputs = X_train.shape[1]\n",
    "n_outputs = Y_train.shape[1]\n",
    "\n",
    "Theta = np.random.randn(n_inputs, n_outputs)\n",
    "\n",
    "eta = 0.01\n",
    "n_epochs = 5001\n",
    "m = len(X_train)\n",
    "\n",
    "patience = 50\n",
    "best_loss = np.inf\n",
    "epochs_without_improvement = 0\n",
    "best_theta = None\n",
    "\n",
    "print(\"Memulai training...\")\n",
    "for epoch in range(n_epochs):\n",
    "    logits = X_train.dot(Theta)\n",
    "    Y_proba = softmax(logits)\n",
    "\n",
    "    error = Y_proba - Y_train\n",
    "    gradients = (1/m) * X_train.T.dot(error)\n",
    "    \n",
    "    Theta = Theta - eta * gradients\n",
    "    \n",
    "    val_logits = X_val.dot(Theta)\n",
    "    val_proba = softmax(val_logits)\n",
    "    val_loss = cross_entropy_loss(val_proba, Y_val)\n",
    "    \n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch: {epoch}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_theta = Theta.copy()\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        \n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"Early stopping di epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "print(\"\\nTraining selesai.\")\n",
    "print(f\"Loss validasi terbaik: {best_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
